{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kZtWy9l8iWB",
        "outputId": "c26bd5b9-034e-4971-b5af-a82b5c16b2df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.4/132.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q google-generativeai chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "import google.generativeai as genai\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# ==================== CONFIGURATION ====================\n",
        "\n",
        "class RAGConfig:\n",
        "    \"\"\"Configuration for RAG system\"\"\"\n",
        "    GEMINI_MODEL = \"gemini-2.5-flash-lite\"\n",
        "    EMBEDDING_MODEL = \"models/text-embedding-004\"\n",
        "    CHUNK_SIZE = 500\n",
        "    CHUNK_OVERLAP = 50 # Chunk1 and Chunk 2 will have 50 char similar\n",
        "    TOP_K_RESULTS = 3\n",
        "    COLLECTION_NAME = \"rag_documents\"\n",
        "\n",
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get(\"key_1\")\n",
        "# Set your API key  # Replace with your actual API key\n",
        "genai.configure(api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "bLEqe39k9b-_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTextLoader:\n",
        "    \"\"\"Custom loader for reading .txt files\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_file(file_path: str) -> str:\n",
        "        \"\"\"Load text from a single .txt file\"\"\"\n",
        "        if not file_path.endswith('.txt'):\n",
        "            raise ValueError(\"Only .txt files are supported\")\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        return content"
      ],
      "metadata": {
        "id": "uetk0ZYt-jrw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@staticmethod\n",
        "def load_directory(directory_path: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"Load all .txt files from a directory\"\"\"\n",
        "        documents = []\n",
        "\n",
        "        for filename in os.listdir(directory_path):\n",
        "            if filename.endswith('.txt'):\n",
        "                file_path = os.path.join(directory_path, filename)\n",
        "                content = CustomTextLoader.load_file(file_path)\n",
        "                documents.append({\n",
        "                    'content': content,\n",
        "                    'source': filename,\n",
        "                    'path': file_path\n",
        "                })\n",
        "\n",
        "        return documents"
      ],
      "metadata": {
        "id": "TCfK9GOsATYX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== CUSTOM TEXT SPLITTER ====================\n",
        "\n",
        "class CustomTextSplitter:\n",
        "    \"\"\"Custom text splitter with overlap support\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "    def split_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into chunks with overlap\"\"\"\n",
        "        # Clean the text\n",
        "        text = self._clean_text(text)\n",
        "\n",
        "        # Split by sentences first for better semantic boundaries\n",
        "        sentences = self._split_into_sentences(text)\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_length = len(sentence)\n",
        "\n",
        "            if current_length + sentence_length > self.chunk_size and current_chunk:\n",
        "                # Save current chunk\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "\n",
        "                # Create overlap by keeping last few sentences\n",
        "                overlap_text = ' '.join(current_chunk)\n",
        "                overlap_sentences = []\n",
        "                overlap_length = 0\n",
        "\n",
        "                for s in reversed(current_chunk):\n",
        "                    if overlap_length + len(s) <= self.chunk_overlap:\n",
        "                        overlap_sentences.insert(0, s)\n",
        "                        overlap_length += len(s)\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                current_chunk = overlap_sentences\n",
        "                current_length = overlap_length\n",
        "\n",
        "            current_chunk.append(sentence)\n",
        "            current_length += sentence_length\n",
        "\n",
        "        # Add the last chunk\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    @staticmethod\n",
        "    def _clean_text(text: str) -> str:\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        # Remove special characters but keep punctuation\n",
        "        text = re.sub(r'[^\\w\\s.,!?;:\\-\\'\\\"()]', '', text)\n",
        "        return text.strip()\n",
        "\n",
        "    @staticmethod\n",
        "    def _split_into_sentences(text: str) -> List[str]:\n",
        "        \"\"\"Split text into sentences\"\"\"\n",
        "        # Simple sentence splitter\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        return [s.strip() for s in sentences if s.strip()]"
      ],
      "metadata": {
        "id": "IY7WZbyCAXk3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==================== EMBEDDING MANAGER ====================\n",
        "\n",
        "class EmbeddingManager:\n",
        "    \"\"\"Manage embeddings using Gemini's embedding model\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = RAGConfig.EMBEDDING_MODEL):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def embed_text(self, text: str) -> List[float]:\n",
        "        \"\"\"Generate embedding for a single text\"\"\"\n",
        "        result = genai.embed_content(\n",
        "            model=self.model_name,\n",
        "            content=text,\n",
        "            task_type=\"retrieval_document\"\n",
        "        )\n",
        "        return result['embedding']\n",
        "\n",
        "    def embed_query(self, query: str) -> List[float]:\n",
        "        \"\"\"Generate embedding for a query\"\"\"\n",
        "        result = genai.embed_content(\n",
        "            model=self.model_name,\n",
        "            content=query,\n",
        "            task_type=\"retrieval_query\"\n",
        "        )\n",
        "        return result['embedding']\n",
        "\n",
        "    def embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for multiple texts\"\"\"\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            embedding = self.embed_text(text)\n",
        "            embeddings.append(embedding)\n",
        "        return embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "cW60ii_ZBCZS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== EMBEDDING MANAGER ====================\n",
        "\n",
        "class EmbeddingManager:\n",
        "    \"\"\"Manage embeddings using Gemini's embedding model\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = RAGConfig.EMBEDDING_MODEL):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def embed_text(self, text: str) -> List[float]:\n",
        "        \"\"\"Generate embedding for a single text\"\"\"\n",
        "        result = genai.embed_content(\n",
        "            model=self.model_name,\n",
        "            content=text,\n",
        "            task_type=\"retrieval_document\"\n",
        "        )\n",
        "        return result['embedding']\n",
        "\n",
        "    def embed_query(self, query: str) -> List[float]:\n",
        "        \"\"\"Generate embedding for a query\"\"\"\n",
        "        result = genai.embed_content(\n",
        "            model=self.model_name,\n",
        "            content=query,\n",
        "            task_type=\"retrieval_query\"\n",
        "        )\n",
        "        return result['embedding']\n",
        "\n",
        "    def embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for multiple texts\"\"\"\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            embedding = self.embed_text(text)\n",
        "            embeddings.append(embedding)\n",
        "        return embeddings\n"
      ],
      "metadata": {
        "id": "BEbu10YMDtxI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== VECTOR STORE ====================\n",
        "class VectorStore:\n",
        "    \"\"\"ChromaDB vector store manager\"\"\"\n",
        "\n",
        "    def __init__(self, collection_name: str = RAGConfig.COLLECTION_NAME):\n",
        "        self.client = chromadb.Client(Settings(\n",
        "            anonymized_telemetry=False,\n",
        "            allow_reset=True\n",
        "        ))\n",
        "        self.collection_name = collection_name\n",
        "        self.collection = None\n",
        "        self.embedding_manager = EmbeddingManager()\n",
        "\n",
        "    def create_collection(self, reset: bool = False):\n",
        "        \"\"\"Create or get collection\"\"\"\n",
        "        if reset:\n",
        "            try:\n",
        "                self.client.delete_collection(name=self.collection_name)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        self.collection = self.client.get_or_create_collection(\n",
        "            name=self.collection_name,\n",
        "            metadata={\"description\": \"RAG document embeddings\"}\n",
        "        )\n",
        "\n",
        "    def add_documents(self, chunks: List[str], metadatas: List[Dict]):\n",
        "        \"\"\"Add document chunks to the vector store\"\"\"\n",
        "        if not self.collection:\n",
        "            self.create_collection()\n",
        "\n",
        "        print(f\"Generating embeddings for {len(chunks)} chunks...\")\n",
        "        embeddings = self.embedding_manager.embed_batch(chunks)\n",
        "\n",
        "        # Generate IDs\n",
        "        ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
        "\n",
        "        print(f\"Adding {len(chunks)} chunks to ChromaDB...\")\n",
        "        self.collection.add(\n",
        "            embeddings=embeddings,\n",
        "            documents=chunks,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "        print(\"âœ“ Documents added successfully!\")\n",
        "\n",
        "    def query(self, query_text: str, top_k: int = RAGConfig.TOP_K_RESULTS) -> Dict:\n",
        "        \"\"\"Query the vector store\"\"\"\n",
        "        if not self.collection:\n",
        "            raise ValueError(\"Collection not initialized\")\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embedding = self.embedding_manager.embed_query(query_text)\n",
        "\n",
        "        # Search\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=[query_embedding],\n",
        "            n_results=top_k\n",
        "        )\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "gxJeBVlHD1fL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== RAG SYSTEM ====================\n",
        "\n",
        "class RAGSystem:\n",
        "    \"\"\"Complete RAG system\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.vector_store = VectorStore()\n",
        "        self.text_splitter = CustomTextSplitter(\n",
        "            chunk_size=RAGConfig.CHUNK_SIZE,\n",
        "            chunk_overlap=RAGConfig.CHUNK_OVERLAP\n",
        "        )\n",
        "        self.model = genai.GenerativeModel(RAGConfig.GEMINI_MODEL)\n",
        "\n",
        "    def ingest_documents(self, file_path: str = None, directory_path: str = None, reset: bool = True):\n",
        "        \"\"\"Ingest documents from file or directory\"\"\"\n",
        "        loader = CustomTextLoader()\n",
        "\n",
        "        # Load documents\n",
        "        if file_path:\n",
        "            content = loader.load_file(file_path)\n",
        "            documents = [{'content': content, 'source': os.path.basename(file_path)}]\n",
        "        elif directory_path:\n",
        "            documents = loader.load_directory(directory_path)\n",
        "        else:\n",
        "            raise ValueError(\"Either file_path or directory_path must be provided\")\n",
        "\n",
        "        print(f\"Loaded {len(documents)} document(s)\")\n",
        "\n",
        "        # Split documents into chunks\n",
        "        all_chunks = []\n",
        "        all_metadata = []\n",
        "\n",
        "        for doc in documents:\n",
        "            chunks = self.text_splitter.split_text(doc['content'])\n",
        "            print(f\"Split {doc['source']} into {len(chunks)} chunks\")\n",
        "\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                all_chunks.append(chunk)\n",
        "                all_metadata.append({\n",
        "                    'source': doc['source'],\n",
        "                    'chunk_id': i,\n",
        "                    'total_chunks': len(chunks)\n",
        "                })\n",
        "\n",
        "        # Create collection and add documents\n",
        "        self.vector_store.create_collection(reset=reset)\n",
        "        self.vector_store.add_documents(all_chunks, all_metadata)\n",
        "\n",
        "        print(f\"\\nâœ“ Ingestion complete! Total chunks: {len(all_chunks)}\")\n",
        "\n",
        "    def query(self, question: str, top_k: int = RAGConfig.TOP_K_RESULTS) -> Tuple[str, List[Dict]]:\n",
        "        \"\"\"Query the RAG system\"\"\"\n",
        "        # Retrieve relevant documents\n",
        "        results = self.vector_store.query(question, top_k=top_k)\n",
        "\n",
        "        # Extract context\n",
        "        contexts = results['documents'][0]\n",
        "        metadatas = results['metadatas'][0]\n",
        "\n",
        "        # Build context string\n",
        "        context_str = \"\\n\\n\".join([\n",
        "            f\"[Source: {meta['source']}, Chunk {meta['chunk_id']+1}/{meta['total_chunks']}]\\n{doc}\"\n",
        "            for doc, meta in zip(contexts, metadatas)\n",
        "        ])\n",
        "\n",
        "        # Create prompt\n",
        "        prompt = f\"\"\"Answer the question based on the context provided.\n",
        "        If the answer cannot be found in the context,\n",
        "        say \"I cannot answer this based on the provided context.\"\n",
        "\n",
        "Context:\n",
        "{context_str}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        # Generate response\n",
        "        response = self.model.generate_content(prompt)\n",
        "\n",
        "        # Prepare source information\n",
        "        sources = [\n",
        "            {\n",
        "                'source': meta['source'],\n",
        "                'chunk_id': meta['chunk_id'],\n",
        "                'text': doc[:200] + \"...\"\n",
        "            }\n",
        "            for doc, meta in zip(contexts, metadatas)\n",
        "        ]\n",
        "\n",
        "        return response.text, sources\n"
      ],
      "metadata": {
        "id": "92MVD26kD_YF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==================== INTERACTIVE MODE ====================\n",
        "\n",
        "def interactive_mode():\n",
        "    \"\"\"Run RAG system in interactive mode\"\"\"\n",
        "    print(\" Starting RAG System in Interactive Mode...\")\n",
        "    rag = RAGSystem()\n",
        "\n",
        "    # Get file path from user\n",
        "    file_path = input(\"\\nEnter path to .txt file (or press Enter to use sample): \").strip()\n",
        "\n",
        "    if not file_path:\n",
        "        # Create and use sample file\n",
        "        print(\"Using sample document...\")\n",
        "        with open('sample_ai.txt', 'w', encoding='utf-8') as f:\n",
        "            f.write(\"\"\"\n",
        "            Artificial Intelligence is revolutionizing technology. Machine learning enables\n",
        "            computers to learn from data. Deep learning uses neural networks for complex tasks.\n",
        "            \"\"\")\n",
        "        file_path = 'sample_ai.txt'\n",
        "\n",
        "    # Ingest documents\n",
        "    rag.ingest_documents(file_path=file_path)\n",
        "\n",
        "    # Interactive query loop\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" RAG System Ready! Type 'quit' or 'exit' to stop.\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nâ“ Your question: \").strip()\n",
        "\n",
        "        if question.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\" Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if not question:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            answer, sources = rag.query(question)\n",
        "            print(f\"\\n Answer:\\n{answer}\\n\")\n",
        "            print(\"ğŸ“„ Sources:\")\n",
        "            for i, source in enumerate(sources, 1):\n",
        "                print(f\"  {i}. {source['source']} (Chunk {source['chunk_id']+1})\")\n",
        "        except Exception as e:\n",
        "            print(f\" Error: {str(e)}\")\n",
        "\n",
        "# Uncomment to run interactive mode:\n",
        "interactive_mode()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "XB6upSCgFMLw",
        "outputId": "a8d6f2ab-3fdb-4d03-85d9-0d1dbf2a5047"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Starting RAG System in Interactive Mode...\n",
            "\n",
            "Enter path to .txt file (or press Enter to use sample): \n",
            "Using sample document...\n",
            "Loaded 1 document(s)\n",
            "Split sample_ai.txt into 1 chunks\n",
            "Generating embeddings for 1 chunks...\n",
            "Adding 1 chunks to ChromaDB...\n",
            "âœ“ Documents added successfully!\n",
            "\n",
            "âœ“ Ingestion complete! Total chunks: 1\n",
            "\n",
            "======================================================================\n",
            " RAG System Ready! Type 'quit' or 'exit' to stop.\n",
            "======================================================================\n",
            "\n",
            "â“ Your question: what is ai\n",
            "\n",
            " Answer:\n",
            "Artificial Intelligence is revolutionizing technology.\n",
            "\n",
            "ğŸ“„ Sources:\n",
            "  1. sample_ai.txt (Chunk 1)\n",
            "\n",
            "â“ Your question: who is virat\n",
            "\n",
            " Answer:\n",
            "I cannot answer this based on the provided context.\n",
            "\n",
            "ğŸ“„ Sources:\n",
            "  1. sample_ai.txt (Chunk 1)\n",
            "\n",
            "â“ Your question: what is computer vision\n",
            "\n",
            " Answer:\n",
            "I cannot answer this based on the provided context.\n",
            "\n",
            "ğŸ“„ Sources:\n",
            "  1. sample_ai.txt (Chunk 1)\n",
            "\n",
            "â“ Your question: what is nlp\n",
            "\n",
            " Answer:\n",
            "I cannot answer this based on the provided context.\n",
            "\n",
            "ğŸ“„ Sources:\n",
            "  1. sample_ai.txt (Chunk 1)\n",
            "\n",
            "â“ Your question: quit\n",
            " Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzrsqWwpF9Cs",
        "outputId": "f12d3108-53b8-47be-9dc4-e99b260e4027"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== EXAMPLE USAGE ====================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Example usage of the RAG system\"\"\"\n",
        "\n",
        "    # Initialize RAG system\n",
        "    print(\"ğŸš€ Initializing RAG System...\")\n",
        "    rag = RAGSystem()\n",
        "\n",
        "    # Example 1: Create a sample text file\n",
        "    print(\"\\nğŸ“ Creating sample document...\")\n",
        "    sample_text = \"\"\"\n",
        "    Artificial Intelligence (AI) is transforming the world. Machine learning, a subset of AI,\n",
        "    enables computers to learn from data without explicit programming. Deep learning uses neural\n",
        "    networks with multiple layers to process complex patterns.\n",
        "\n",
        "    Natural Language Processing (NLP) is a branch of AI that helps computers understand human language.\n",
        "    It powers applications like chatbots, translation services, and sentiment analysis.\n",
        "\n",
        "    Computer vision is another important AI field that enables machines to interpret visual information.\n",
        "    It's used in facial recognition, autonomous vehicles, and medical imaging.\n",
        "\n",
        "    The future of AI includes advancements in quantum computing, edge AI, and explainable AI systems.\n",
        "    \"\"\"\n",
        "\n",
        "    with open('sample_ai.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write(sample_text)\n",
        "\n",
        "    # Example 2: Ingest the document\n",
        "    print(\"\\nğŸ“š Ingesting documents...\")\n",
        "    rag.ingest_documents(file_path='sample_ai.txt')\n",
        "\n",
        "    # Example 3: Query the system\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ğŸ’¬ RAG SYSTEM READY - Ask questions!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    questions = [\n",
        "        \"What is machine learning?\",\n",
        "        \"What are the applications of NLP?\",\n",
        "        \"What is the future of AI?\"\n",
        "    ]"
      ],
      "metadata": {
        "id": "FKa6K2csGhf_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dH6aLmKmHLJp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}