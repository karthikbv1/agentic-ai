{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH25aXASfNZd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWN27pNafNsZ"
      },
      "source": [
        "##Day 2 Hands-On Lab:\n",
        "Mastering Prompt Engineering: This notebook covers the practical application of advanced Prompt Engineering frameworks like R.O.L.E.S. and Chain of Thought (CoT).\n",
        "\n",
        "We will compare the results of poorly-structured prompts against engineered prompts using LangChain and two powerful models: OpenAI (GPT-4o) and Gemini model\n",
        "\n",
        "Setup & Environment Configuration: We need to install the necessary libraries and set up our API keys.\n",
        "\n",
        "We recommend using Colab Secrets for secure storage of your OPENAI_API_KEY and GEMINI_API_KEY.#\n",
        "\n",
        "Install LangChain and necessary dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.3.7 langchain-core==0.3.15 langchain-google-genai==2.0.4 google-generativeai==0.8.3 protobuf==5.27.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fej1feB5n0kn",
        "outputId": "f68d2290-d6d1-4dfc-a4ad-5931b3f68cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.3.7\n",
            "  Downloading langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core==0.3.15\n",
            "  Downloading langchain_core-0.3.15-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-google-genai==2.0.4\n",
            "  Downloading langchain_google_genai-2.0.4-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting google-generativeai==0.8.3\n",
            "  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting protobuf==5.27.0\n",
            "  Downloading protobuf-5.27.0-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.7) (6.0.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.7) (2.0.44)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.7) (3.13.2)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.7)\n",
            "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.3.7)\n",
            "  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy<2.0.0,>=1.26.0 (from langchain==0.3.7)\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.7) (2.12.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.7) (2.32.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.7) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core==0.3.15) (1.33)\n",
            "Collecting packaging<25,>=23.2 (from langchain-core==0.3.15)\n",
            "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core==0.3.15) (4.15.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.3) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.3) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.3) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.3) (2.43.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.3) (4.67.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai==0.8.3) (1.26.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (1.22.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai==0.8.3) (1.72.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai==0.8.3) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai==0.8.3) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai==0.8.3) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.15) (3.0.0)\n",
            "INFO: pip is looking at multiple versions of langchain-text-splitters to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.7)\n",
            "  Downloading langchain_text_splitters-0.3.10-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading langchain_text_splitters-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading langchain_text_splitters-0.3.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-text-splitters to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_text_splitters-0.3.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.7) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.7) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.7) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.7) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.7) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.7) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.7) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.7) (3.2.4)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai==0.8.3) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai==0.8.3) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai==0.8.3) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai==0.8.3) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai==0.8.3) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai==0.8.3) (3.2.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai==0.8.3) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (1.3.1)\n",
            "Downloading langchain-0.3.7-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.15-py3-none-any.whl (408 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.0.4-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_generativeai-0.8.3-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.8/160.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.27.0-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.2/309.2 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, packaging, numpy, langsmith, langchain-core, langchain-text-splitters, langchain, google-generativeai, langchain-google-genai\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.4.47\n",
            "    Uninstalling langsmith-0.4.47:\n",
            "      Successfully uninstalled langsmith-0.4.47\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.1.0\n",
            "    Uninstalling langchain-core-1.1.0:\n",
            "      Successfully uninstalled langchain-core-1.1.0\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 1.1.0\n",
            "    Uninstalling langchain-1.1.0:\n",
            "      Successfully uninstalled langchain-1.1.0\n",
            "  Attempting uninstall: google-generativeai\n",
            "    Found existing installation: google-generativeai 0.8.2\n",
            "    Uninstalling google-generativeai-0.8.2:\n",
            "      Successfully uninstalled google-generativeai-0.8.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 5.27.0 which is incompatible.\n",
            "langgraph-prebuilt 1.0.5 requires langchain-core>=1.0.0, but you have langchain-core 0.3.15 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-generativeai-0.8.3 langchain-0.3.7 langchain-core-0.3.15 langchain-google-genai-2.0.4 langchain-text-splitters-0.3.2 langsmith-0.1.147 numpy-1.26.4 packaging-24.2 protobuf-5.27.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy",
                  "packaging"
                ]
              },
              "id": "c3f5ea756d904c2a9a47c5381e313130"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ufdvkWEfraa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from google.colab import userdata\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tdwZq43ftEe"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- API Key Setup ---\n",
        "# Load environment variables (assumes using Colab Secrets or a .env file)\n",
        "# If using Colab Secrets, click the key icon on the left panel.\n",
        "# Variables must be named OPENAI_API_KEY and GEMINI_API_KEY.\n",
        "\n",
        "# Get API keys from environment variables\n",
        "\n",
        "gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "if not gemini_api_key:\n",
        "    print(\"Warning: GEMINI_API_KEY not found. Gemini model will not run.\")\n",
        "\n",
        "# openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "# if not openai_api_key:\n",
        "#     print(\"Warning: OPENAI_API_KEY not found. OpenAI model will not run.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSLM9G7xf6iK",
        "outputId": "c00ccd6b-b8f3-4da9-cdd2-d3bf925a5781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Models initialized.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# --- Model Initialization ---\n",
        "# 1. OpenAI Model\n",
        "# Using a powerful model like gpt-4o for complex tasks\n",
        "# openai_model = ChatOpenAI(\n",
        "#     model=\"gpt-4o-mini\",\n",
        "#     api_key=openai_api_key,\n",
        "#     temperature=0.3\n",
        "# )\n",
        "\n",
        "# 2. Google Gemini Model\n",
        "# Using gemini-2.5-flash for a highly performant and cost-effective alternative\n",
        "gemini_model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-lite-001\",\n",
        "    google_api_key=gemini_api_key,\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "print(\"Setup complete. Models initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc8EQJNNgUWu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8jwpp48gVCy"
      },
      "source": [
        "\n",
        "\n",
        "2. Advanced Reasoning: Chain of Thought (CoT) The CoT technique involves adding the phrase \"Let's think step by step\" to the prompt. This forces the LLM to structure its reasoning before providing a final answer, dramatically improving accuracy in logic and mathematical problems.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llGJaPrEgjhe",
        "outputId": "63d7d150-1fcb-4a84-fb6f-bfd425a0ff20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- GPT Standard Response ---\n",
            "If all but 9 goats run away, that means 9 goats are left.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "user_query = \"\"\"\n",
        "A farmer has 17 goats. All but 9 run away. How many goats are left?\n",
        "\"\"\"\n",
        "\n",
        "# --- Standard Prompt  ---\n",
        "standard_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"user\", \"{query}\")\n",
        "])\n",
        "\n",
        "chain_standard = standard_prompt | gemini_model | StrOutputParser()\n",
        "print(\"--- GPT Standard Response ---\")\n",
        "print(chain_standard.invoke({\"query\": user_query}))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIeh4zIhCk2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd8b010-4d71-4c59-dc50-b77d0e69f402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- GPT-4o CoT Response (Correct) ---\n",
            "Here's how to solve the problem:\n",
            "\n",
            "*   The problem states \"All but 9 run away.\"\n",
            "*   This means 9 goats did *not* run away.\n",
            "\n",
            "Answer: 9\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Chain of Thought (CoT) Prompt (Reasoning) ---\n",
        "cot_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"user\", \"Let's think step by step to find the correct answer. {query}\")\n",
        "])\n",
        "chain_cot = cot_prompt | gemini_model | StrOutputParser()\n",
        "print(\"\\n--- GPT-4o CoT Response (Correct) ---\")\n",
        "print(chain_cot.invoke({\"query\": user_query}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vrsxs1x8glan"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvXa3hZDg4sf"
      },
      "source": [
        "**3. The R.O.L.E.S. Prompt Engineering Framework : **\n",
        "\n",
        "R.O.L.E.S. ensures every critical aspect of the desired output is explicitly defined, reducing ambiguity and improving consistency.\n",
        "\n",
        "Role: The identity the LLM adopts (e.g., expert, beginner).Sets expertise,tone, and knowledge boundaries.\n",
        "\n",
        "Objective: The core task to accomplish (e.g., summarize, critique, generate).Defines the purpose of the output.\n",
        "\n",
        "Limitations: Constraints on length, format, or content (e.g., max 50 words, exclude jargon).Ensures practical, usable results.\n",
        "\n",
        "Examples: Few-shot learning examples (Input $\\to$ Output pairs).Guides model on specific required style or format.\n",
        "\n",
        "Style: Tone, language complexity, and output format (e.g., professional, JSON, Markdown).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jPNtISJj-Cd"
      },
      "source": [
        "## Example 1 : Write a blog post"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rybGcxVFjzLG",
        "outputId": "220a4250-b41f-4ce7-e009-2bc82a286272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---  PROMPT (Gemini) ---\n",
            "## The Robots are Coming (and They're Already Here): A Look at the Ever-Evolving World of AI\n",
            "\n",
            "For years, we've been bombarded with headlines about Artificial Intelligence. From self-driving cars to algorithms that recommend our next binge-worthy show, AI has steadily crept into our lives, transforming the way we work, play, and interact with the world. But what exactly *is* AI, and what does the future hold for this rapidly evolving technology?\n",
            "\n",
            "**Decoding the Buzzwords: What is AI, Really?**\n",
            "\n",
            "Let's start with the basics. Artificial Intelligence, at its core, refers to the ability of a computer or a robot controlled by a computer to perform tasks that typically require human intelligence. This can range from simple tasks like recognizing patterns to complex ones like making decisions, learning from experience, and even creating art.\n",
            "\n",
            "Think of it like this:\n",
            "\n",
            "*   **Narrow or Weak AI:** This is the type of AI we see most often today. It's designed to perform a specific task, like playing chess (Deep Blue) or recognizing faces (facial recognition software).\n",
            "*   **General or Strong AI:** This is the holy grail of AI research. It refers to AI that possesses human-level intelligence and can perform any intellectual task that a human being can. We're not quite there yet!\n",
            "*   **Super AI:** This is a hypothetical level of AI that surpasses human intelligence in every aspect. This is the realm of science fiction, but it's a concept that fuels much of the discussion around AI's potential impact.\n",
            "\n",
            "**AI in Action: Where is it Making a Difference?**\n",
            "\n",
            "AI is no longer a futuristic fantasy; it's a present-day reality. Here are just a few examples of how AI is already shaping our world:\n",
            "\n",
            "*   **Healthcare:** AI is being used to diagnose diseases, personalize treatments, and accelerate drug discovery.\n",
            "*   **Finance:** AI algorithms are used for fraud detection, risk assessment, and algorithmic trading.\n",
            "*   **Manufacturing:** Robots and AI-powered systems are automating production lines, improving efficiency, and reducing costs.\n",
            "*   **Transportation:** Self-driving cars are becoming increasingly sophisticated, promising to revolutionize the way we travel.\n",
            "*   **Entertainment:** AI powers recommendation engines, creates personalized content, and even generates music and art.\n",
            "\n",
            "**The Challenges and Opportunities of an AI-Driven Future**\n",
            "\n",
            "The rise of AI presents both exciting opportunities and significant challenges.\n",
            "\n",
            "**Opportunities:**\n",
            "\n",
            "*   **Increased Efficiency and Productivity:** AI can automate repetitive tasks, freeing up humans to focus on more creative and strategic work.\n",
            "*   **Improved Decision-Making:** AI can analyze vast amounts of data to provide insights and support better decision-making.\n",
            "*   **New Discoveries and Innovations:** AI can accelerate scientific research and lead to breakthroughs in various fields.\n",
            "*   **Enhanced Accessibility:** AI can help bridge the gap for people with disabilities, providing assistive technologies and personalized support.\n",
            "\n",
            "**Challenges:**\n",
            "\n",
            "*   **Job Displacement:** Automation could lead to job losses in certain industries, requiring us to adapt and reskill the workforce.\n",
            "*   **Bias and Fairness:** AI algorithms can reflect the biases present in the data they are trained on, leading to unfair or discriminatory outcomes.\n",
            "*   **Ethical Concerns:** Issues like privacy, data security, and the potential for misuse of AI technologies need careful consideration.\n",
            "*   **The \"Black Box\" Problem:** Some AI systems are complex and difficult to understand, making it hard to explain their decisions and build trust.\n",
            "\n",
            "**The Future is Now: What Can We Expect?**\n",
            "\n",
            "The future of AI is uncertain, but one thing is clear: it will continue to evolve at a rapid pace. We can expect to see:\n",
            "\n",
            "*   **More sophisticated AI systems:** AI will become more capable of performing complex tasks and interacting with the world in more human-like ways.\n",
            "*   **Greater integration into our daily lives:** AI will become even more embedded in the products and services we use.\n",
            "*   **Increased focus on ethical considerations:** As AI becomes more powerful, the need for responsible development and deployment will become even more critical.\n",
            "\n",
            "**The Bottom Line:**\n",
            "\n",
            "AI is a powerful force that is already reshaping our world. By understanding the technology, its potential, and its challenges, we can navigate this new era with greater awareness and prepare ourselves for the future. The robots may be coming, but they're already here, and it's up to us to shape their impact on our lives.\n",
            "\n",
            "**What are your thoughts on AI? Share your comments and questions below!**\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def generate_roles_prompt(role, objective, limitations, style, query):\n",
        "    \"\"\"Generates a structured prompt based on the R.O.L.E.S. framework.\"\"\"\n",
        "    prompt_template = f\"\"\"\n",
        "    --- INSTRUCTIONS (R.O.L.E.S. FRAMEWORK) ---\n",
        "    ROLE: {role}\n",
        "    OBJECTIVE: {objective}\n",
        "    LIMITATIONS: {limitations}\n",
        "    STYLE: {style}\n",
        "    --- USER QUERY ---\n",
        "    {query}\n",
        "    \"\"\"\n",
        "    return prompt_template\n",
        "\n",
        "# Define a simple chain template for easy switching between models\n",
        "def create_roles_chain(model):\n",
        "    prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "    return prompt | model | StrOutputParser()\n",
        "\n",
        "\n",
        "blog_prompt = \"Write a blog post about AI.\"\n",
        "print(\"---  PROMPT (Gemini) ---\")\n",
        "print(create_roles_chain(gemini_model).invoke({\"input\": blog_prompt}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4aYZG6vkRwO",
        "outputId": "3a482484-3394-4e0a-93c8-008de03597a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ENGINEERED PROMPT (Gemini 2.5 Flash) ---\n",
            "## Unlock the Power of Language Models: Introducing R.O.L.E.S.\n",
            "\n",
            "Alright, marketing mavens and tech enthusiasts! Are you ready to supercharge your content creation, streamline your campaigns, and unlock previously unimaginable levels of efficiency? Then buckle up, because we're about to dive into the revolutionary world of prompt engineering – and I'm thrilled to introduce you to a framework that will change the way you interact with language models forever: **R.O.L.E.S.**\n",
            "\n",
            "Forget vague instructions and frustrating results. R.O.L.E.S. provides a structured approach to crafting prompts that elicit *exactly* what you need. Think of it as a secret weapon, a cheat code, a finely tuned instrument for extracting brilliance from these powerful engines.\n",
            "\n",
            "**R.O.L.E.S.** stands for:\n",
            "\n",
            "*   **R**ole: Define the persona you want the model to adopt (e.g., a seasoned copywriter, a witty social media guru).\n",
            "*   **O**bjective: Clearly state the desired outcome (e.g., generate five compelling headlines, write a product description).\n",
            "*   **L**ength: Specify the desired output length (e.g., 100 words, a bulleted list).\n",
            "*   **E**xample: Provide a concrete example of the desired style or format.\n",
            "*   **S**tyle: Dictate the tone, voice, and stylistic elements (e.g., humorous, formal, concise).\n",
            "\n",
            "By systematically applying these elements, you'll gain unprecedented control over the output, ensuring it aligns perfectly with your brand and objectives. Get ready to witness a paradigm shift in your workflow. Let's get started!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# --- R.O.L.E.S. ENGINEERED PROMPT ---\n",
        "roles_blog_prompt = generate_roles_prompt(\n",
        "    role=\"A seasoned Marketing Director and AI expert.\",\n",
        "    objective=\"Write a compelling, 300-word introduction to the R.O.L.E.S. prompt engineering framework.\",\n",
        "    limitations=\"Maximum 300 words. Must not use the word 'chatbot' or 'AI helper'.\",\n",
        "    style=\"Enthusiastic, engaging, and structured as Markdown with a H2 title.\",\n",
        "    query=\"Write the post now.\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- ENGINEERED PROMPT (Gemini 2.5 Flash) ---\")\n",
        "print(create_roles_chain(gemini_model).invoke({\"input\": roles_blog_prompt}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XrTp6kaj3gE"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI1LMOqAk68K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1fKMGPak7ZD"
      },
      "source": [
        "#Prompt Set 2: Code Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axaun9LklKTg",
        "outputId": "f9afc886-60af-439e-c729-d37c859afa2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- PROMPT (Gemini 2.5 Flash) ---\n",
            "```python\n",
            "def sum_list(l):\n",
            "  \"\"\"\n",
            "  Calculates the sum of all numbers in a list.\n",
            "\n",
            "  Args:\n",
            "    l: A list of numbers.\n",
            "\n",
            "  Returns:\n",
            "    The sum of the numbers in the list.\n",
            "  \"\"\"\n",
            "  total = 0\n",
            "  for i in l:\n",
            "    total += i\n",
            "  return total\n",
            "```\n",
            "\n",
            "**What was wrong (and the fixes):**\n",
            "\n",
            "The original code was perfectly functional and correct.  There was nothing inherently *wrong* with it.  However, I've added:\n",
            "\n",
            "*   **Docstring:**  A docstring (the text within triple quotes `\"\"\"...\"\"\"`) is crucial for good code.  It explains what the function does, what arguments it takes, and what it returns.  This makes the code much easier to understand and use.  It's good practice.\n",
            "*   **Comments (Optional):**  While the code is simple enough that comments aren't strictly *needed*, in more complex functions, comments can clarify the logic.\n",
            "\n",
            "The core logic of the original code (initializing `total` to 0, iterating through the list, and adding each element to `total`) is correct and efficient.  No changes to that were needed.\n",
            "\n",
            "\n",
            "--- ENGINEERED PROMPT (Gemini) ---\n",
            "```python\n",
            "def sum_list(l):\n",
            "    \"\"\"Calculates the sum of all numbers in a list.\n",
            "\n",
            "    Args:\n",
            "        l: A list of numbers (int or float).\n",
            "\n",
            "    Returns:\n",
            "        The sum of all numbers in the input list.\n",
            "    \"\"\"\n",
            "    return sum(l)\n",
            "```\n",
            "\n",
            "Critique: The original code, while functionally correct, is inefficient. It iterates through the list manually to calculate the sum. The refactored code leverages Python's built-in `sum()` function, which is optimized for this specific task. This approach is more concise, readable, and generally faster. There were no logical bugs in the original code, but the refactoring significantly improves its efficiency and adheres to best practices for Python development.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "code_snippet = \"def sum_list(l): total = 0; for i in l: total += i; return total\"\n",
        "\n",
        "\n",
        "# ---  PROMPT TEST ---\n",
        "code_prompt = f\"Fix this code and tell me what is wrong: {code_snippet}\"\n",
        "print(\"--- PROMPT (Gemini 2.5 Flash) ---\")\n",
        "print(create_roles_chain(gemini_model).invoke({\"input\": code_prompt}))\n",
        "\n",
        "\n",
        "# --- R.O.L.E.S. ENGINEERED PROMPT ---\n",
        "roles_code_prompt = generate_roles_prompt(\n",
        "    role=\"A Senior Python Developer specializing in clean, efficient code.\",\n",
        "    objective=\"Refactor the provided code snippet for efficiency (using built-in methods), add a docstring, and identify a single major bug in the logic (if any).\",\n",
        "    limitations=\"Do not change the function name. Output the revised code first, then provide a single paragraph critique.\",\n",
        "    style=\"Formal tone. Output must be in two sections: 'Revised Code' and 'Critique'.\",\n",
        "    query=f\"Here is the code: {code_snippet}\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- ENGINEERED PROMPT (Gemini) ---\")\n",
        "print(create_roles_chain(gemini_model).invoke({\"input\": roles_code_prompt}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnnsy1qzlO9c"
      },
      "source": [
        "# Prompt Set 3: Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8KBfWTFlfJK",
        "outputId": "a560b3ae-e0ea-4396-a2f0-48f7d0a379a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ENGINEERED PROMPT (Gemini 2.5 Flash) ---\n",
            "Alright, let's dissect this week's AAPL charade. The stock exhibited a rather uninspiring sideways drift, failing to convincingly break either the 170 or 175 resistance levels. Volume remained tepid throughout the period, suggesting a lack of conviction from either bulls or bears. The 50-day moving average continues to act as a mild support, but the Relative Strength Index (RSI) hovers around a neutral 55, indicating a lack of strong momentum. Furthermore, the stochastic oscillator is currently showing a potential bearish crossover, which, coupled with the stagnant price action, hints at a possible short-term pullback towards the 168 level. I maintain a cautious outlook; the market appears indecisive, and any significant move will likely require a catalyst we haven't yet seen.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "roles_finance_prompt = generate_roles_prompt(\n",
        "    role=\"A highly skeptical Senior Financial Analyst.\",\n",
        "    objective=\"Analyze the past week's fictional performance of AAPL stock based on technical analysis, and make a plausible, short-term prediction.\",\n",
        "    limitations=\"Do not use real-time search. Analysis must be a single paragraph.\",\n",
        "    style=\"Highly technical, formal, and use the term 'stochastic' at least once.\",\n",
        "    query=\"Perform the analysis.\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- ENGINEERED PROMPT (Gemini 2.5 Flash) ---\")\n",
        "print(create_roles_chain(gemini_model).invoke({\"input\": roles_finance_prompt}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1gW5OfFlrO-"
      },
      "source": [
        "# Prompt Set 4: Summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI9pb1ljl583",
        "outputId": "cb27cbbf-a0dd-4286-a773-fc6eb6e247d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NORMAL PROMPT (Gemini model) ---\n",
            "Acme Corp. announced strong Q3 earnings on October 15th, with revenue increasing by $45 million, exceeding last year's. The company attributes this growth to a successful digital marketing campaign and plans to increase hiring by 20% starting January 1st. The positive news led to a $5 per share increase in stock price within two days.\n",
            "\n",
            "\n",
            "--- ENGINEERED PROMPT (Gemini 2.5 Flash) ---\n",
            "Acme Corp reported Q3 earnings on October 15th, with revenue increasing to $45 million, exceeding the previous year's $30 million. Hiring is projected to increase by 20% starting January 1st.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "long_text = \"\"\"\n",
        "The company, Acme Corp, announced its Q3 earnings on October 15th, reporting a staggering\n",
        "revenue increase of $45 million, surpassing last year's figure of $30 million.\n",
        "The CEO stated that hiring will increase by 20% in the new year, starting January 1st.\n",
        "This growth is primarily attributed to their successful digital marketing campaign launched in July.\n",
        "The stock reacted positively, climbing $5 per share within 48 hours of the announcement.\n",
        "\"\"\"\n",
        "\n",
        "# ---  PROMPT TEST ---\n",
        "bad_summarize_prompt = f\"Summarize this paragraph: {long_text}\"\n",
        "print(\"--- NORMAL PROMPT (Gemini model) ---\")\n",
        "print(create_roles_chain(gemini_model).invoke({\"input\": bad_summarize_prompt}))\n",
        "\n",
        "\n",
        "# --- R.O.L.E.S. ENGINEERED PROMPT ---\n",
        "roles_summarize_prompt = generate_roles_prompt(\n",
        "    role=\"A News Editor reviewing a financial wire report.\",\n",
        "    objective=\"Summarize the text, focusing only on reported dates and financial figures.\",\n",
        "    limitations=\"Must be exactly 2 sentences long. Include no opinion or analysis.\",\n",
        "    style=\"Objective and journalistic.\",\n",
        "    query=f\"Summarize this text: {long_text}\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- ENGINEERED PROMPT (Gemini 2.5 Flash) ---\")\n",
        "print(create_roles_chain(gemini_model).invoke({\"input\": roles_summarize_prompt}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byGOIbtlmDhO"
      },
      "source": [
        "Prompt Set 5: Topic Classification\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k93Mi6ZkmObz",
        "outputId": "a1ed6a80-6a38-4114-84d0-95d4c31b2a35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- BAD PROMPT (Gemini 2.5 Flash) ---\n",
            "This text is about **a positive change in the performance of a computer's graphics card (GPU)**. Specifically:\n",
            "\n",
            "*   **Driver updates:** Software updates for the GPU.\n",
            "*   **Fixed latency issues:** Resolved delays or lag in processing data.\n",
            "*   **Improved frame rates substantially:** Made the game or application run smoother and faster, leading to a better visual experience.\n",
            "\n",
            "In short, the text describes how updating the GPU's drivers has improved its performance.\n",
            "\n",
            "--- ENGINEERED PROMPT (GPT-4o) ---\n",
            "Hardware\n"
          ]
        }
      ],
      "source": [
        "\n",
        "classification_text = \"The latest driver updates fixed the latency issues on the GPU, improving frame rates substantially.\"\n",
        "\n",
        "# --- NORMAL/BAD PROMPT TEST ---\n",
        "bad_classify_prompt = f\"What is this text about: {classification_text}\"\n",
        "print(\"--- BAD PROMPT (Gemini 2.5 Flash) ---\")\n",
        "print(create_roles_chain(gemini_model).invoke({\"input\": bad_classify_prompt}))\n",
        "\n",
        "\n",
        "# --- R.O.L.E.S. ENGINEERED PROMPT (Using Examples for format) ---\n",
        "roles_classify_prompt = generate_roles_prompt(\n",
        "    role=\"A Data Scientist performing classification.\",\n",
        "    objective=\"Classify the following text into one of these categories: [Hardware, Software, Finance].\",\n",
        "    limitations=\"Output must be a single word and case-sensitive (e.g., 'Hardware').\",\n",
        "    style=\"Raw text output. Use the following example:\",\n",
        "    query=f\"\"\"\n",
        "    Example Input: \"The market closed high.\"\n",
        "    Example Output: Finance\n",
        "\n",
        "    Classify this input: \"{classification_text}\"\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- ENGINEERED PROMPT (GPT-4o) ---\")\n",
        "print(create_roles_chain(openai_model).invoke({\"input\": roles_classify_prompt}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8mq7xNqml8G"
      },
      "source": [
        "Prompt Set 6: Sentiment Analysis\n",
        "\n",
        "Determine if the customer sentiment is positive, neutral, or negative.\n",
        "\n",
        "Output must be a JSON object with keys sentiment and confidence (1-100)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UViLmIghfH8H",
        "outputId": "bfd79562-4dbd-4750-e0ab-676361c6bf5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ENGINEERED PROMPT (GPT-4o) ---\n",
            "{\n",
            "  \"sentiment\": \"Negative\",\n",
            "  \"confidence\": 85\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "sentiment_review = \"The product works exactly as advertised, but the delivery took far too long, making the overall experience frustrating.\"\n",
        "\n",
        "\n",
        "# --- R.O.L.E.S. ENGINEERED PROMPT ---\n",
        "roles_sentiment_prompt = generate_roles_prompt(\n",
        "    role=\"A Customer Service Manager focused on customer feedback.\",\n",
        "    objective=\"Analyze the provided review text to determine overall customer sentiment (Positive, Negative, or Neutral).\",\n",
        "    limitations=\"Only use the exact labels: 'Positive', 'Negative', or 'Neutral'.\",\n",
        "    style=\"Strictly output a JSON object.\",\n",
        "    query=f\"\"\"\n",
        "    Analyze this review: \"{sentiment_review}\"\n",
        "\n",
        "    Required JSON format:\n",
        "    {{\n",
        "      \"sentiment\": \"<LABEL>\",\n",
        "      \"confidence\": \"<1-100 score>\"\n",
        "    }}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- ENGINEERED PROMPT (GPT-4o) ---\")\n",
        "print(create_roles_chain(openai_model).invoke({\"input\": roles_sentiment_prompt}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We8oL-zMlMMY"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}